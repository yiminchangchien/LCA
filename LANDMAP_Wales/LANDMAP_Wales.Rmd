---
title: Short Paper
author:
  - name: Alice Anonymous
    email: alice@example.com
    affiliation: Some Institute of Technology
    footnote: 1
  - name: Bob Security
    email: bob@example.com
    affiliation: Another University
  - name: Cat Memes
    email: cat@example.com
    affiliation: Another University
    footnote: 2
  - name: Derek Zoolander
    email: derek@example.com
    affiliation: Some Institute of Technology
    footnote: 2
address:
  - code: Some Institute of Technology
    address: Department, Street, City, State, Zip
  - code: Another University
    address: Department, Street, City, State, Zip
footnote:
  - code: 1
    text: "Corresponding Author"
  - code: 2
    text: "Equal contribution"
abstract: |
  This is the abstract.

  It consists of two paragraphs.

journal: "An awesome journal"
date: "`r Sys.Date()`"
bibliography: mybibfile.bib
#linenumbers: true
#numbersections: true
csl: elsevier-harvard.csl
output: rticles::elsevier_article
---

_Text based on elsarticle sample manuscript, see [http://www.elsevier.com/author-schemas/latex-instructions#elsarticle](http://www.elsevier.com/author-schemas/latex-instructions#elsarticle)_


The Elsevier article class
==========================

#### Installation

If the document class *elsarticle* is not available on your computer,
you can download and install the system package *texlive-publishers*
(Linux) or install the LaTeX package *elsarticle* using the package
manager of your TeX installation, which is typically TeX Live or MikTeX.

#### Data Preparation
```{r}
library(dplyr)
library(raster)
library(sf)
library(sp)
library(readr)
library(rgeos)

##### 1. collect data
###### 1.1. download the visual and sensory aspect polygons for Wales
temp <- tempfile()
temp2 <- tempfile()
download.file("http://lle.gov.wales/catalogue/item/LandmapVisualSensory.zip", temp)
unzip(zipfile = temp, exdir = temp2)
"NRW_LandMap_Visual_SensoryPolygon.shp" %>%
  file.path(temp2, .) %>% 
  st_read(stringsAsFactors = FALSE) ->
  LCA
rm(list = c("temp", "temp2"))

###### 1.2. download Scenic-Or-Not dataset
bng <- "+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 
        +ellps=airy +datum=OSGB36 +units=m +no_defs"
wgs84 <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

sc <- 
  read_tsv("http://scenicornot.datasciencelab.co.uk/votes.tsv",
           col_types = cols("ID" = col_number(),
                            "Lat" = col_double(),
                            "Lon" = col_double(),
                            "Average" = col_double(),
                            "Variance" = col_double(),
                            "Votes" = col_character(),
                            "Geograph URI" = col_character())) %>%
  as.data.frame %>%
  st_as_sf(coords=c("Lon","Lat"), crs=4326) %>%
  st_transform(crs=27700) %>%
  as("Spatial")

# #### combining Geograph metadata
# Geograph <- function(url, pts) {
#   pts <-
#     url %>% 
#     url() %>%
#     gzcon() %>%
#     readLines() %>%
#     textConnection() %>%
#     read.csv(sep = "\t") %>%
#     as.data.frame() %>%
#     left_join(pts, ., by = "gridimage_id")
#   return(pts)
# }
# sc <- 
#   read_tsv("http://scenicornot.datasciencelab.co.uk/votes.tsv",
#            col_types = cols("ID" = col_number(),
#                             "Lat" = col_double(),
#                             "Lon" = col_double(),
#                             "Average" = col_double(),
#                             "Variance" = col_double(),
#                             "Votes" = col_character(),
#                             "Geograph URI" = col_character())) %>%
#   as.data.frame %>%
#   # st_as_sf(coords=c("Lon","Lat"), crs=4326) %>%
#   # st_transform(crs=27700) %>%
#   mutate(gridimage_id = as.integer(gsub("http://www.geograph.org.uk/photo/", "", `Geograph URI`))) %>%
#   Geograph("http://data.geograph.org.uk/dumps/gridimage_geo.tsv.gz", .) ->
#   test
#   as("Spatial")


###### 1.3. load in the wildness components
setwd("/Users/Yi-Min/Rsession/ScenicOrNot/predictor variables/Wilderness Dimensions")
#wildness <- list("access","naturalness","remoteness","ruggedness") %>% lapply(raster)

#test <- lapply(list, raster)
     "access" %>% raster -> Abs
"naturalness" %>% raster -> Nat
 "remoteness" %>% raster -> Rem
 "ruggedness" %>% raster -> Rug

Wales.sp <- 
  raster::getData("GADM", country = "United Kingdom", level = 1) %>%
  subset(NAME_1 == "Wales") %>%
  spTransform(crs(Abs))

Abs[mask(is.na(Abs), rgeos::gBuffer(Wales.sp, byid=T, width=-100))] <- 19.125
LCA.sp$Sce <- over(LCA.sp, sc[,'Average'], fn = median) %>% as.vector() %>% unlist()
LCA.sp$Abs <- raster::extract(Abs, LCA.sp, fun = median, na.rm = TRUE) %>% as.vector()
LCA.sp$Nat <- raster::extract(Nat, LCA.sp, fun = median, na.rm = TRUE) %>% as.vector()
LCA.sp$Rem <- raster::extract(Rem, LCA.sp, fun = median, na.rm = TRUE) %>% as.vector()
LCA.sp$Rug <- raster::extract(Rug, LCA.sp, fun = median, na.rm = TRUE) %>% as.vector()
```
## Analysis
#### Oridnal regression model

```{r}
####### 3. Build a global ordinal logistic regression model
##### 3.1. Base model (only one predictor i.e. Scenic-Or-Not)
require(MASS)
model = polr(SQ ~ Sce, data = LCATrain, Hess = TRUE)
summary(model)

#### 3.2.1. Mapping the residual
LCA.sp$fitted <- 
  colnames(model$fitted.values)[max.col(model$fitted.values, ties.method="first")] %>%
  factor(., levels = c("Low", "Moderate", "High", "Outstanding"), ordered = TRUE)
```
#### 4. Building a Gradient Boosting Machine

```{r}
library(caret)
##### 4.1. Split data into training and validation dataset
set.seed(2046)
trainIndex <- createDataPartition(LCA.sp$SQ, p = .7, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)
LCATrain <- LCA.sf[ trainIndex,]
LCATest  <- LCA.sf[-trainIndex,]

##### 4.2. Build a Gradient Boosting Machine
library(gbm)
LCATrain$SQ = factor(LCATrain$SQ, levels = c("Low", "Moderate", "High", "Outstanding"), ordered = FALSE)
LCA_gbm <- gbm(SQ ~ Sce + Abs + Nat, # + Rem + Rug,
               data = st_drop_geometry(LCATrain),
               distribution = "multinomial",
               #cv.folds = 4,
               shrinkage = .01,
               n.minobsinnode = 10,
               n.trees = 1000) 
summary(LCA_gbm)
print(LCA_gbm)

###### 4.3. validate the model
pred = predict.gbm(object = LCA_gbm,
                   newdata = LCATest,
                   n.trees = 1000,
                   type = "response")
labels = colnames(pred)[apply(pred, 1, which.max)]
LCATest <- cbind(LCATest, fitted = labels)

setwd("/Users/Yi-Min/Rsession/ScenicOrNot/scenicness/Results/Figures")
png(filename = "test.png", w = 10, h = 10, units = "in", res = 300)
par(mar=c(0,0,0,0)) 
spplot(LCATest[,'fitted'], col = NA)
dev.off()

result = data.frame(LCATest$SQ, labels)
print(result)

# confusion matrix
cm = confusionMatrix(LCATest$SQ, as.factor(labels))
print(cm)
```

```{r}
# predicut the scenic quality based on the predictive model of all the covariates
pred = predict.gbm(object = LCA_gbm,
                   newdata = OSGB.1km,
                   n.trees = 1000,
                   type = "response")
labels = colnames(pred)[apply(pred, 1, which.max)]
result = data.frame(LCATest$SQ, labels)
print(result)


## Reference
# https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab
# https://www.datatechnotes.com/2018/03/classification-with-gradient-boosting.html
# confusion matrix terminology:https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/

# Classification with caret train method
tc = trainControl(method = "repeatedcv", number = 10)

model = train(SQ ~ Sce + Abs + Nat, data=LCATrain, method="gbm", trControl=tc)

pred = predict(model, LCATest)
result = data.frame(LCATest$SQ, pred)
print(result)

cm = confusionMatrix(LCATest$SQ, as.factor(pred))
print(cm)



```

```{r}
GB.sp <- 
  raster::getData("GADM", country = "United Kingdom", level = 1) %>%
  subset(NAME_1 == c("England","Scotland","Wales")) %>%
  spTransform(crs(Abs))

Abs[mask(is.na(Abs), rgeos::gBuffer(GB.sp, byid=T, width=-100))] <- 19.125

# LCA.sp <- 
#   LCA %>%
#   select("UID","VS_46") %>%
#   as(., "Spatial")

# names(LCA.sp)[2] <- "SQ"

crs(OSGB.1km)
crs(sc)
crs(Abs)

OSGB.1km$Sce <- over(OSGB.1km, sc[,'Average'], fn = median) %>% as.vector() %>% unlist()
OSGB.1km$Abs <- raster::extract(Abs, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Nat <- raster::extract(Nat, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Rem <- raster::extract(Rem, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Rug <- raster::extract(Rug, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()

```


Once the package is properly installed, you can use the document class
*elsarticle* to create a manuscript. Please make sure that your
manuscript follows the guidelines in the Guide for Authors of the
relevant journal. It is not necessary to typeset your manuscript in
exactly the same way as an article, unless you are submitting to a
camera-ready copy (CRC) journal.

#### Analysis
```{r}

```
```{r}
```
apply the model to a case study area and suggest a character area by grouping adjacent pixels with the same class. I suppose this would have to be on a 1km grid for the Scenicness data
```{r}
st_read("/Users/Yi-Min/Rsession/ScenicOrNot/scenicness/Grid/OSGB_Grid/Shapefile/OSGB_Grid_1km.shp") %>%
  as("Spatial") %>%
  spTransform(crs(Abs)) ->
  OSGB.1km 

sc %>% spTransform(crs(Abs)) -> sc  

OSGB.1km$Sce <- over(OSGB.1km, sc[,'Average'], fn = median) %>% as.vector() %>% unlist()
OSGB.1km$Abs <- raster::extract(Abs, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Nat <- raster::extract(Nat, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Rem <- raster::extract(Rem, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Rug <- raster::extract(Rug, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()


```

-   document style

-   baselineskip

-   front matter

-   keywords and MSC codes

-   theorems, definitions and proofs

-   lables of enumerations

-   citation style and labeling.

Front matter
============

The author names and affiliations could be formatted in two ways:

(1) Group the authors per affiliation.

(2) Use footnotes to indicate the affiliations.

See the front matter of this document for examples. You are recommended
to conform your choice to the journal you are submitting to.

Bibliography styles
===================

There are various bibliography styles available. You can select the
style of your choice in the preamble of this document. These styles are
Elsevier styles based on standard styles like Harvard and Vancouver.
Please use BibTeXÂ to generate your bibliography and include DOIs
whenever available.

Here are two sample references: @Feynman1963118 [@Dirac1953888].

References {#references .unnumbered}
==========
