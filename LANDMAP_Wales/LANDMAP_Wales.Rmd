---
title: An hybrid approach of identifying outstanding scenic beauty areas by machine learning andscape designation
author:
  - name: Yi-Min Chang Chien
    email: gyymcc@leeds.ac.uk
    affiliation: School of Geography, University of Leeds
    footnote: 1
  - name: Alexis Comber
    email: bob@example.com
    affiliation: School of Geography, University of Leeds
  - name: Steve Carber
    email: cat@example.com
    affiliation: School of Geography, University of Leeds
    footnote: 2
address:
  - code: Some Institute of Technology
    address: Department, Street, City, State, Zip
  - code: Another University
    address: Department, Street, City, State, Zip
footnote:
  - code: 1
    text: "Corresponding Author"
  - code: 2
    text: "Equal contribution"
abstract: |
  This is the abstract.

  It consists of two paragraphs.

journal: "Landscape and Urban Planning"
date: "`r Sys.Date()`"
bibliography: mybibfile.bib
link-citations: yes
#linenumbers: true
#numbersections: true
csl: elsevier-harvard.csl
output: rticles::elsevier_article
---

_Text based on elsarticle sample manuscript, see [http://www.elsevier.com/author-schemas/latex-instructions#elsarticle](http://www.elsevier.com/author-schemas/latex-instructions#elsarticle)_


The Elsevier article class
==========================

#### Introduction
Landscape designation --> designating criteria --> a few expert judgement --> to some extent, landscape designation is based on character --> Landscape Character Assessment (descriptive v.s. evaluative) --> evaluative consideration (i.e. how is the evaluative components under current character system?)

- In order to make these judgements (some of which are subjective) in a transparent and consistent way, this Guidance sets out which criteria Natural England intends to use.

- It is certain in the Guidance for AONB designation that judgement has to be made in order to value the landscape;

- No information is provided on how to involve stakeholders and how to measure whether there is consensus about the value of the landscape or not;

- The single use of experts in assessment methods is highly contested in landscape preference research;

- The practicality of the existing framework is questionable.

- Nevertheless, questions need to be raised here. First of all, who is describing value to the landscape? Is the designation based on public judgment, or on expert judgement? This is not being stressed in the document, only it states that: ‘where there is a consensus of opinion that an area meets the statutory criteria or should be designated, this helps determining whether it is accorded a special value that should be recognized. Views of stakeholders and the public can be strong indicators as to whether there is consensus about the value of a landscape’. Unfortunately, no information is provided on how to involve stakeholders and how to measure whether there is consensus about the value of the landscape or not.

- Characterisation of landscape is a matter of interpretation and perception;

- When it comes to planning, decisions have to be made, so ignoring its importance by not valuing historic landscapes seems impractical;

- Landscape is perceived by people, but as perception is influenced by personal feelings and opinions, we cannot be objective.

- the landscapes are classified on classified of low, moderate, high or outstanding quality.
# qualitative to quantitative evaluation
The current practice of LCA is based on qualitative descriptions. The evaluation is carried out by specialists and recorded in a qualitative manner where the evaluative components under current character system includes wildness/integrity/ The relationship between wildness and scenicness was evidence by our previous attempt (forthcoming). Thus, it might be worth to look at whether the qualitative measure of wildness quality can be enhanced the qualitative evaluation of scenic quality. the sources involved are not quantitative: they are texts containing qualitative descriptions and, as such, need to be analysed using a mix of approaches that
combine spatial analysis with close reading.
- Some of the criteria used to value the landscape are also used as landscape character indica-tors for perceptual aspects such as tranquillity and wildness.  However, it is not stated how to use these criteria in practice or what to do with these indicators in the first place. It is merely mentioned that they can be used to identify valued landscapes that merit some form of designa-tion or recognition.  Yet, in what way are the stakeholders involved? How can we derive the value that stakeholders assign to the landscape from these criteria? This does not seem to be explained in the LCA document. 
# sentence
- the point on the east bank of Windermere, for instance, gives little indication of precisely to where on this large body of water the text is referring
- 1:100,000, National land use policy: deciding which areas to protect
- 1: 25,000, Monitoring regional land use Local management: Managing sensitive areas
- lt must be possible for decision-making bodies to update the information on their areas easily and quickly.
- budgetary constraints and time limits.
- The maps can be fairly easily updated on a regular basis.
- The results of certain statistical surveys (areolar surveys of land use/cover) might suggest that the problem of the unit area is not really problematical. However, it must be remembered that these surveys sample points, not surfaces. Nonetheless, in the field it is still possible to classify a point as an item in a land cover nomenclature
- it must provide an acceptable representation of reality.
-
-
-

[@Seresinhe2019]
#### 3. Materials and methods
Why? Welsh landscapes --> The planning authorities intend to use LANDMAP to inform decision making.
### 3.1. Welsh LANDMAP Data
The expert-based landscape qualities LANDMAP is an all-Wales landscape resource where landscape characteristics, qualities and influences on the landscape are recorded and evaluated  in a nationally consistent data set.

5 landscape characteristics/characterisations based on geology, habitats, visually sensory, historic and cultural character layers: 
- geological landscape (classification)
- landscape habitats (classification
- visual & sensory (classification): maps landscape characteristics and qualities as perceived through our senses, primarily visually. --> The physical attributes of landform and land cover, their visible patterns and their interrelationship. --> Examples of information recorded includes: Landscape elements and features, aesthetic presence of water and seasonality, important
qualities, boundary type, scale, enclosure, diversity,
colour, sense of place, night time light, building
materials, views, scenic quality, rarity, integrity and
character.
  • built land
  • coastal
  • coastal waters
  • developed unbuilt land
  • exposed upland/plateau
  • flat lowland/levels
  • hills, lower plateau & scarp slopes
  • inland water (including associated edge)
  • lowland valleys
  • rolling lowland
  • upland valleys
- historic landscape
-
LANDMAP is managed as a spatial dataset
• Spatial data consists of observations (surveys in
LANDMAP) with locations (mapped geographic areas)
• Geographical Information System (GIS) software records
and displays this spatial data

Maps and classifies landscapes
– Describes their characteristics, qualities and components

The Welsh landscape baseline LANDMAP data : Visual and Sensory aspect

#### 3.2. Scenic-Or-Not data 
is on 1-10 scores. The crowdsourced scenic ratings were reclassified into one of scenic quality classes to correspond to the LANDMAP dataset.
```{r}
library(dplyr)
library(raster)
library(readr)
library(rgeos)
library(sf)
library(sp)
library(tidyverse)
library(xgboost)

##### 1. collect data
###### 1.1. download the visual and sensory aspect polygons for Wales
temp <- tempfile()
temp2 <- tempfile()
download.file("http://lle.gov.wales/catalogue/item/LandmapVisualSensory.zip", temp)
unzip(zipfile = temp, exdir = temp2)
"NRW_LandMap_Visual_SensoryPolygon.shp" %>%
  file.path(temp2, .) %>% 
  st_read(stringsAsFactors = FALSE) ->
  LCA
rm(list = c("temp", "temp2"))

LCA.sp <- 
  LCA %>%
  dplyr::select("UID",SQ = "VS_46") %>%
  as("Spatial")

###### 1.2. download Scenic-Or-Not dataset
bng <- "+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 
        +ellps=airy +datum=OSGB36 +units=m +no_defs"
wgs84 <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

sc <- 
  read_tsv("http://scenicornot.datasciencelab.co.uk/votes.tsv",
           col_types = cols("ID" = col_number(),
                            "Lat" = col_double(),
                            "Lon" = col_double(),
                            "Average" = col_double(),
                            "Variance" = col_double(),
                            "Votes" = col_character(),
                            "Geograph URI" = col_character())) %>%
  as.data.frame %>%
  st_as_sf(coords=c("Lon","Lat"), crs=4326) %>%
  st_transform(crs=27700) %>%
  as("Spatial")

#### combining Geograph metadata
Geograph <- function(url, pts) {
  pts <-
    url %>%
    url() %>%
    gzcon() %>%
    readLines() %>%
    textConnection() %>%
    read.csv(sep = "\t") %>%
    as.data.frame() %>%
    left_join(pts, ., by = "gridimage_id")
  return(pts)
}
sc <-
  read_tsv("http://scenicornot.datasciencelab.co.uk/votes.tsv",
           col_types = cols("ID" = col_number(),
                            "Lat" = col_double(),
                            "Lon" = col_double(),
                            "Average" = col_double(),
                            "Variance" = col_double(),
                            "Votes" = col_character(),
                            "Geograph URI" = col_character())) %>%
  as.data.frame %>%
  # st_as_sf(coords=c("Lon","Lat"), crs=4326) %>%
  # st_transform(crs=27700) %>%
  mutate(gridimage_id = as.integer(gsub("http://www.geograph.org.uk/photo/", "", `Geograph URI`))) %>%
  Geograph("http://data.geograph.org.uk/dumps/gridimage_geo.tsv.gz", .) %>%
  
  Geograph("http://data.geograph.org.uk/dumps/gridimage_geo.tsv.gz", .) 
  sc_tab %>%
  drop_na(viewpoint_eastings, viewpoint_northings) %>%
  st_as_sf(coords=c("viewpoint_eastings","viewpoint_northings"), crs=27700) %>%
  as("Spatial") -> 
  sc_new

sc.merge <- read.xlsx("/Users/Yi-Min/Rsession/ScenicOrNot/ScenicOrNot dataset/ScenicOrNot_Geograph_v2.xlsx")

sc_new[Wales.sp, ] %>%
  as.data.frame() %>%
  Geograph("http://data.geograph.org.uk/dumps/gridimage_text.tsv.gz", .) ->
  test

sc_tab2 <- openxlsx::read.xlsx("/Users/Yi-Min/Rsession/ScenicOrNot/ScenicOrNot dataset/ScenicOrNot_Geograph_v2.xlsx")

Geograph("http://data.geograph.org.uk/dumps/gridimage_geo.tsv.gz", .) 



###### 1.3. load in the wildness components
setwd("/Users/Yi-Min/Rsession/ScenicOrNot/predictor variables/Wilderness Dimensions")
#wildness <- list("access","naturalness","remoteness","ruggedness") %>% lapply(raster)

#test <- lapply(list, raster)
"absence.asc" %>% raster -> Abs
"naturalness" %>% raster -> Nat
 "remoteness" %>% raster -> Rem
 "ruggedness" %>% raster -> Rug

GB.sp <- 
  raster::getData("GADM", country = "United Kingdom", level = 1) %>%
  subset(NAME_1 %in% c("England", "Scotland", "Wales")) %>%
  st_as_sf() %>%
  group_by(NAME_0) %>%
  summarise() %>%
  as("Spatial") %>%
  spTransform(crs(Abs))

Abs[mask(is.na(Abs), rgeos::gBuffer(GB.sp, byid=T, width=-100))] <- 19.125
writeRaster(Abs, filename = file.path("/Users/Yi-Min/Rsession/ScenicOrNot/predictor variables/Wilderness Dimensions", "absence"), format="ascii", overwrite=TRUE)
crs(LCA.sp)
crs(sc)
crs(sc_new)
crs(Abs)
crs(Nat)
crs(Rem)
crs(Rug)

LCA.sp <- LCA.sp %>% spTransform(crs(Abs))
    sc <- sc %>% spTransform(crs(Abs))
sc_new <- sc_new %>% spTransform(crs(Abs))

LCA.sp$Sce <- over(LCA.sp, sc[,'Average'], fn = median) %>% as.vector() %>% unlist()
LCA.sp$SC  <- over(LCA.sp, sc_new[,'Average'], fn = median) %>% as.vector() %>% unlist()
LCA.sp$Abs <- raster::extract(Abs, LCA.sp, fun = median, na.rm = TRUE) %>% as.vector()
LCA.sp$Nat <- raster::extract(Nat, LCA.sp, fun = median, na.rm = TRUE) %>% as.vector()
LCA.sp$Rem <- raster::extract(Rem, LCA.sp, fun = median, na.rm = TRUE) %>% as.vector()
LCA.sp$Rug <- raster::extract(Rug, LCA.sp, fun = median, na.rm = TRUE) %>% as.vector()
```

```{r}
sc_new$SQ <- over(sc_new, LCA.sp[,"SQ"]) %>% unlist()
sc_new$Abs <- raster::extract(Abs, sc_new)
sc_new$Nat <- raster::extract(Nat, sc_new)
sc_new$Rem <- raster::extract(Rem, sc_new)
sc_new$Rug <- raster::extract(Rug, sc_new)

sc.Wales <- 
  sc_new[Wales.sp, ] %>%
  st_as_sf() %>%
  dplyr::select(SQ, SC=Average, Abs, Nat, Rem, Rug)

summary(sc.Wales)

sc.Wales$SQ <- factor(sc.Wales$SQ, levels = c("Low", "Moderate", "High", "Outstanding"), ordered = TRUE) 

plot(sc.Wales$SQ, sc.Wales$SC, main="Fig 1. Boxplot of Scenic-Or-Not ratings vesus four SCENIC QUALITY")
plot(sc.Wales$SQ, sc.Wales$Abs, main="Fig 2. Boxplot of Absence vesus four SCENIC QUALITY")
plot(sc.Wales$SQ, sc.Wales$Nat, main="Fig 3. Boxplot of Absence vesus four SCENIC QUALITY")
plot(sc.Wales$SQ, sc.Wales$Rem, main="Fig 4. Boxplot of Absence vesus four SCENIC QUALITY")
plot(sc.Wales$SQ, sc.Wales$Rug, main="Fig 5. Boxplot of Absence vesus four SCENIC QUALITY")

plot(sc.Wales$SQ, main="Fig 1. Distribution of SCENIC QUALITY")
hist(sc.Wales$SC, main="Fig 2. Distribution of SCENIC QUALITY")
hist(sc.Wales$Abs, main="Fig 3. Distribution of Absence")
hist(sc.Wales$Nat, main="Fig 4. Distribution of Naturalness")
hist(log(sc.Wales$Rem), main="Fig 5. Distribution of Remoteness")
hist(log(sc.Wales$Rug), main="Fig 6. Distribution of Ruggedness")

library(caret)
##### 4.1. Split data into training and validation dataset
set.seed(2046)
trainIndex <- 
  sc.Wales[-c(which(sc.Wales$SQ=="Unassessed"), which(is.na(sc.Wales$SQ))),] %>%
  as.data.frame() %>%
  .$SQ %>%
  createDataPartition(p = .8, 
                      list = FALSE, 
                      times = 1)
head(trainIndex)
LCATrain <- 
  sc.Wales[-c(which(sc.Wales$SQ=="Unassessed"), which(is.na(sc.Wales$SQ))),] %>%
  st_as_sf() %>%
  .[ trainIndex,]

LCATest  <- 
  sc.Wales[-c(which(sc.Wales$SQ=="Unassessed"), which(is.na(sc.Wales$SQ))),] %>% 
  st_as_sf() %>% 
  .[-trainIndex,]

### Using gbm package
LCATrain$SQ = factor(LCATrain$SQ, levels = c("Low", "Moderate", "High", "Outstanding", "Unassessed", "NA"), ordered = FALSE)
set.seed(2046)
require(gbm)
LCA_gbm <- 
  LCATrain %>%
  st_drop_geometry() %>%
  #drop_na() %>%
  gbm(SQ ~ SC + Abs + Nat + Rug + Rem,
      distribution = "multinomial",
      data = .,
      #weights = ,
      #var.monotone = NULL,
      n.trees = 2000,
      interaction.depth = 3,
      n.minobsinnode = 15,
      shrinkage = .005,
      #bag.fraction = 0.5, 
      #train.fraction = 1, 
      cv.folds = 30,
      #keep.data = TRUE, 
      #verbose = FALSE, 
      #class.stratify.cv = NULL,
      #n.cores = NULL
      )

#plot(LCA_gbm)
# find index for number trees with minimum CV error
best <- which.min(LCA_gbm$cv.error)

# get MSE and compute RMSE
sqrt(LCA_gbm$cv.error[best])

gbm.perf(LCA_gbm, method = "cv")

###### 4.3. validate the model
pred = predict.gbm(object = LCA_gbm,
                   newdata = LCATest,
                   n.trees = 2000,
                   type = "response")
labels = colnames(pred)[apply(pred, 1, which.max)]
LCATest <- cbind(LCATest, fitted = labels)
head(LCATest)

# setwd("/Users/Yi-Min/Rsession/ScenicOrNot/scenicness/Results/Figures")
# png(filename = "test.png", w = 10, h = 10, units = "in", res = 300)
# par(mar=c(0,0,0,0)) 
# plot(LCATest[,'fitted'], col = NA)
# dev.off()

result = data.frame(LCATest$SQ, labels)
print(result)

# confusion matrix
cm = confusionMatrix(as.factor(LCATest$SQ), as.factor(labels))
print(cm)

```
## Analysis
#### Oridnal Regression Model

```{r}
####### 3. Build a global ordinal logistic regression model
##### 3.1. Base model (only one predictor i.e. Scenic-Or-Not)
require(MASS)
model = polr(SQ ~ Sce, data = LCATrain, Hess = TRUE)
summary(model)

# #### 3.2.1. Mapping the residual
# LCA.sp$fitted <- 
#   colnames(model$fitted.values)[max.col(model$fitted.values, ties.method="first")] %>%
#   factor(., levels = c("Low", "Moderate", "High", "Outstanding"), ordered = TRUE)

###### 4.3. validate the model
pred = predict(object = model,
                   newdata = LCATest,
                   n.trees = 2000,
                   type = "response")
labels = colnames(pred)[apply(pred, 1, which.max)]
LCATest <- cbind(LCATest, fitted = labels)
head(LCATest)

# setwd("/Users/Yi-Min/Rsession/ScenicOrNot/scenicness/Results/Figures")
# png(filename = "test.png", w = 10, h = 10, units = "in", res = 300)
# par(mar=c(0,0,0,0)) 
# plot(LCATest[,'fitted'], col = NA)
# dev.off()

result = data.frame(LCATest$SQ, labels)
print(result)

# confusion matrix
cm = confusionMatrix(as.factor(LCATest$SQ), as.factor(labels))
print(cm)

```
#### 4. Building a Gradient Boosting Machine
### hyper-parameters tuning 
# shrinkage: learning rate determine how quickly the algorithm adapts, which is usually a small positive value between 0 and 1, where decreases lead to slower fitting, thus requiring the user to increase K
# n.minobsinnode: η the fraction of data that is used at each iterative step / the minimum number of training set samples in a node to commence splitting
# n.trees: number of trees/number of iterations: generally adding more trees to the model can be very slow to overfit. The advice is to keep adding trees until no further improvement is observed.


```{r}
##### grouping the categories of the response vairable

LCA.sp$SQ1 <- LCA.sp$SQ
LCA.sp$SQ2 <- LCA.sp$SQ
LCA.sp$SQ3 <- LCA.sp$SQ
LCA.sp$SQ4 <- LCA.sp$SQ

# case 1: combining "Moderate" and "High" as "Middle"
LCA.sp[c(which(LCA.sp$SQ1 == "Moderate"), which(LCA.sp$SQ1 == "High")), "SQ1"] <- "Middle"

# case 2: combining "High" and "Outstanding" as "Scenic"
LCA.sp[c(which(LCA.sp$SQ2 == "High"), which(LCA.sp$SQ2 == "Outstanding")), "SQ2"] <- "Scenic"

# case 3: combining "Low" and "Moderate" as "Non-scenic"
LCA.sp[c(which(LCA.sp$SQ3 == "Low"), which(LCA.sp$SQ3 == "Moderate")), "SQ3"] <- "Non-scenic"

# case 4: combining "High" and "Outstanding" as "Scenic"; "Low" and "Moderate" as "Non-scenic"
LCA.sp[c(which(LCA.sp$SQ4 == "High"), which(LCA.sp$SQ4 == "Outstanding")), "SQ4"] <- "Scenic"
LCA.sp[c(which(LCA.sp$SQ4 == "Low"), which(LCA.sp$SQ4 == "Moderate")), "SQ4"] <- "Non-scenic"

```
#### Exploratory Analysis
```{r}
####### 2. Exploratory analysis
##### 2.1. boxplot LANDMAP Scenic Quality and Scenic-Or-Not ratings
### 2.1.1.
LCA.sp$SQ5 <- LCA.sp$SQ
LCA.sp$SQ5 <- factor(LCA.sp$SQ5, levels = c("Low", "Moderate", "High", "Outstanding"), ordered = TRUE) 

plot(LCA.sp$SQ5, LCA.sp$SC, main="Fig 1. Boxplot of Scenic-Or-Not ratings vesus four SCENIC QUALITY")
plot(LCA.sp$SQ5, LCA.sp$Abs, main="Fig 2. Boxplot of Absence vesus four SCENIC QUALITY")
plot(LCA.sp$SQ5, LCA.sp$Nat, main="Fig 3. Boxplot of Absence vesus four SCENIC QUALITY")
plot(LCA.sp$SQ5, log(LCA.sp$Rem), main="Fig 4. Boxplot of Absence vesus four SCENIC QUALITY")
plot(LCA.sp$SQ5, log(LCA.sp$Rug), main="Fig 5. Boxplot of Absence vesus four SCENIC QUALITY")

plot(LCA.sp$SQ5, main="Fig 1. Distribution of SCENIC QUALITY")
hist(LCA.sp$SC, main="Fig 2. Distribution of SCENIC QUALITY")
hist(LCA.sp$Abs, main="Fig 3. Distribution of Absence")
hist(LCA.sp$Nat, main="Fig 4. Distribution of Naturalness")
hist(log(LCA.sp$Rem), main="Fig 5. Distribution of Remoteness")
hist(log(LCA.sp$Rug), main="Fig 6. Distribution of Ruggedness")

```
### logari Remoteness and Ruggedness
```{r}
LCA.sp$log.Rem <- log(LCA.sp$Rem)
LCA.sp$log.Rug <- log(LCA.sp$Rug)
head(LCA.sp)
```
#### Combining the "Moderate" and "High" categories
```{r}
library(caret)
##### 4.1. Split data into training and validation dataset
set.seed(2046)
trainIndex <- 
  LCA.sp[-c(which(LCA.sp$SQ=="Unassessed"), which(is.na(LCA.sp$SQ))),] %>%
  as.data.frame() %>%
  .$SQ1 %>%
  createDataPartition(p = .8, 
                      list = FALSE, 
                      times = 1)
head(trainIndex)
LCATrain <- 
  LCA.sp[-c(which(LCA.sp$SQ=="Unassessed"), which(is.na(LCA.sp$SQ))),] %>%
  st_as_sf() %>%
  .[ trainIndex,]

LCATest  <- 
  LCA.sp[-c(which(LCA.sp$SQ=="Unassessed"), which(is.na(LCA.sp$SQ))),] %>% 
  st_as_sf() %>% 
  .[-trainIndex,]

# only change the column here
LCATrain$SQ1 = factor(LCATrain$SQ1, levels = c("Low", "Middle", "Outstanding"), ordered = FALSE)
 LCATest$SQ1 = factor(LCATest$SQ1, levels = c("Low", "Middle", "Outstanding"), ordered = FALSE)

LCATrain$SQ2 = factor(LCATrain$SQ2, levels = c("Low", "Moderate", "Scenic"), ordered = FALSE)
 LCATest$SQ2 = factor(LCATest$SQ2, levels = c("Low", "Moderate", "Scenic"), ordered = FALSE)
 
LCATrain$SQ3 = factor(LCATrain$SQ3, levels = c("Non-scenic", "High", "Outstanding"), ordered = FALSE)
 LCATest$SQ3 = factor(LCATest$SQ3, levels = c("Non-scenic", "High", "Outstanding"), ordered = FALSE)
 
LCATrain$SQ4 = factor(LCATrain$SQ4, levels = c("Non-scenic", "Scenic"), ordered = FALSE)
 LCATest$SQ4 = factor(LCATest$SQ4, levels = c("Non-scenic", "Scenic"), ordered = FALSE)
 
x_train <- LCATrain %>% 
  # dplyr::select(SC, Abs, Nat, Rug) %>% #
  dplyr::select(SC, Abs, Nat, log.Rem, log.Rug) %>% 
  st_drop_geometry() %>% 
  as.matrix() %>% 
  xgb.DMatrix()
y_train <- LCATrain$SQ %>% 
  as.integer() %>% 
  -1 %>%
  as.factor()

x_test <- LCATest %>% 
  # dplyr::select(SC, Abs, Nat, Rug) %>% #
  dplyr::select(SC, Abs, Nat, log.Rem, log.Rug) %>%
  st_drop_geometry() %>% 
  as.matrix() %>% 
  xgb.DMatrix()
y_test = LCATest$SQ %>% 
  as.integer() %>% 
  -1

#  train.data <- LCATrain[, predictors] %>% st_drop_geometry() %>% as.matrix()
# train.label <- LCATrain$Sq %>% as.integer() %>% -1
#   test.data <- LCATest[, predictors] %>% st_drop_geometry() %>% as.matrix()
#  test.label <- LCATest$Sq %>% as.integer() %>% -1
# 
# # Transform the two data sets into xgb.Matrix
# xgb.train = xgb.DMatrix(data=train.data, label = train.label)
# xgb.test = xgb.DMatrix(data=test.data, label = test.label)

```

```{r}
library(caret)
##### 4.1. Split data into training and validation dataset
set.seed(2046)
trainIndex <- 
  LCA.sp[-c(which(LCA.sp$SQ=="Unassessed"), which(is.na(LCA.sp$SQ))),] %>%
  as.data.frame() %>%
  .$Sq %>%
  createDataPartition(p = .8, 
                      list = FALSE, 
                      times = 1)
head(trainIndex)
LCATrain <- 
  LCA.sp[-c(which(LCA.sp$SQ=="Unassessed"), which(is.na(LCA.sp$SQ))),] %>%
  st_as_sf() %>%
  .[ trainIndex,]

LCATest  <- 
  LCA.sp[-c(which(LCA.sp$SQ=="Unassessed"), which(is.na(LCA.sp$SQ))),] %>% 
  st_as_sf() %>% 
  .[-trainIndex,]

LCATrain$SQ = factor(LCATrain$SQ, levels = c("Low", "Moderate", "High", "Outstanding"), ordered = FALSE)
LCATest$SQ = factor(LCATest$SQ, levels = c("Low", "Moderate", "High", "Outstanding"), ordered = FALSE)

x_train <- LCATrain %>% 
  dplyr::select(SC, Abs, Nat, Rug) %>% #
  st_drop_geometry() %>% 
  as.matrix() %>% 
  xgb.DMatrix()
y_train <- LCATrain$SQ %>% 
  as.integer() %>% 
  -1 %>%
  as.factor()

x_test <- LCATest %>% 
  dplyr::select(SC, Abs, Nat, Rug) %>%
  st_drop_geometry() %>% 
  as.matrix() %>% 
  xgb.DMatrix()
y_test = LCATest$SQ %>% 
  as.integer() %>% 
  -1

 train.data <- LCATrain[, predictors] %>% st_drop_geometry() %>% as.matrix()
train.label <- LCATrain$SQ %>% as.integer() %>% -1
  test.data <- LCATest[, predictors] %>% st_drop_geometry() %>% as.matrix()
 test.label <- LCATest$SQ %>% as.integer() %>% -1

# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data, label = train.label)
xgb.test = xgb.DMatrix(data=test.data, label = test.label)

```
#3. GBM hyper-parameters tuning

Reference
https://www.sciencedirect.com/science/article/pii/S0378778817320844
# From Kaggle
```{r}
nrounds = 1000
tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = nrounds, by = 100),
  eta = 0.005, #c(0.05, 0.01, 0.005, 0.001),
  max_depth = 5,
  colsample_bytree = 1, # percent of columns to sample from for each tree
  min_child_weight = 1, # minimum node size
  subsample = 0.5,      # percent of training data to sample for each tree
  #gamma = 0
  gamma = 1
  # lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
  # alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000)
)

tune_control <- caret::trainControl(
  method = "cv", # cross-validation
  #method = "repeatedcv",
  #repeats = 2,
  number = 35, # with n folds 
  #index = createFolds(tr_treated$Id_clean), # fix the folds
  verboseIter = FALSE, # no training log
  allowParallel = FALSE, # FALSE for reproducible results
  returnData = FALSE,
  #classProbs = TRUE,
  #summaryFunction = multiClassSummary
  #summaryFunction = twoClassSummary
)

set.seed(2046) 
train_time <- system.time({
  xgb_tune <- caret::train(
    x = x_train,
    y = y_train,
    trControl = tune_control,
    tuneGrid = tune_grid,
    metric = "Accuracy",
    method = "xgbTree",
    # selectionFunction = "best", "oneSE","tolerance",
    # objective = "multi:softprob",
    # eval_metric = "mlogloss",
    num_class = 4
    )
  })
xgb_tune$bestTune
max(xgb_tune$results$Accuracy)

# helper function for the plots
tuneplot <- function(x) {
  ggplot(x) +
    coord_cartesian(ylim = c(max(x$results$Accuracy), min(x$results$Accuracy))) +
    theme_bw()
}

tuneplot(xgb_tune)

# Reference
# https://github.com/topepo/caret/issues/389
```
## for the binary classification
```{r}
fit.xgb <- xgb.train(
  params = list(
    eta = param$eta, 
    max_depth = param$max_depth,
    gamma = param$gamma, 
    colsample_bytree = param$colsample_bytree,
    min_child_weight = param$min_child_weight, 
    subsample = param$subsample
    ),
  data = xgb.trainData, 
  nrounds = param$nrounds, 
  objective = "binary:logistic"
  )

#caret
set.seed(1992)
fit.xgbTree <- train(Class~., data=trainSet, method="xgbTree",
                    metric="Accuracy", trControl=trainControl(method="none"),tuneGrid=param)

#####
###Print results (predictions)
#####
print("xgboost")
predictionxgb <- as.numeric(predict(fit.xgb,  xgb.testData) >= 0.5)
confusionMatrix(predictionxgb,testSet$Class)

```

```{r}
# Model evaluation
#predicted = predict(xgb_tune, x_test)
# Predict outcomes with the test data
xgb.pred <-
  predict(xgb_tune, x_test, reshape=T) %>%
  plyr::mapvalues(from = c(0,1,2), to = c("Low", "Middle", "Outstanding")) %>%
  # plyr::mapvalues(from = c(0,1,2), to = c("Low", "Moderate", "Scenic")) %>%
  # plyr::mapvalues(from = c(0,1,2), to = c("Non-scenic", "High", "Outstanding")) %>%
  data.frame(predict = .)

label = levels(LCATest$SQ1)[y_test+1]
result = data.frame(actual = label, predict = xgb.pred)

# confusion matrix
cm = confusionMatrix(as.factor(result$actual), as.factor(result$predict))
print(cm)

# Reference
# https://datascienceplus.com/extreme-gradient-boosting-with-r/

OSGB.1km.Wales$pred <-
  OSGB.1km.Wales %>%
  st_as_sf() %>%
  st_drop_geometry() %>%
  dplyr::select(Sce, Abs, Nat, Rug) %>%
  predict(xgb_tune, ., reshape=T) %>%
  plyr::mapvalues(from = c(0,1,2,3), to = c("Low", "Moderate", "High", "Outstanding")) %>%
  as.character()

spplot(OSGB.1km.Wales, "pred", col = NA)

```

### Stochastic hyperparameters

### XGBoost ### fit the model with the optimal hyperparameters

```{r}
library(xgboost)
# library(recipes)
# xgb_prep <- recipe(Sale_Price ~ ., data = ames_train) %>%
#   step_integer(all_nominal()) %>%
#   prep(training = ames_train, retain = TRUE) %>%
#   juice()

# Train the XGBoost classifer
set.seed(2046)
LCA_xgb <- xgb.train(
  data = xgb.train,
  objective = "multi:softprob",
  eval_metric = "mlogloss",
  nrounds = 3000,
  nthreads = 8,
  early_stopping_rounds = 5,
  nfold = 10,
  watchlist = list(val1 = xgb.train, val2 = xgb.test),
  params = list(
    booster = "gbtree",
    eta = 0.001,
    max_depth = 3,
    min_child_weight = 3,
    gamma = 3,
    subsample = 0.9,
    colsample_bytree = 1,
    #objective="multi:softprob",
    #eval_metric = "mlogloss",
    num_class = 4),
  verbose = 0
)

# Review the final model and results
LCA_xgb

# Predict outcomes with the test data
xgb.pred = predict(LCA_xgb, test.data, reshape=T) %>% as.data.frame()
#xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(LCATrain$SQ)

# Use the predicted label with the highest probability
xgb.pred$prediction = apply(xgb.pred, 1, function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(LCATest$SQ)[test.label+1]

# Calculate the final accuracy
result = sum(xgb.pred$prediction == xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))


# from https://rstudio-pubs-static.s3.amazonaws.com/336778_d7d321fab8694292bc0531300c11b319.html
LCA_xgb <- xgb.cv(data = xgb.train,
                  nrounds = 3000,
                  nfold = 10,
                  verbose = FALSE,
                  prediction = TRUE,
                  params = list(
                    objective = "multi:softprob",
                    eval_metric = "mlogloss",
                    num_class = 4)
                  )

xgb.pred <- 
  data.frame(LCA_xgb$pred) %>%
  mutate(max_prob = max.col(., ties.method = "last"), label = train.label + 1)
head(xgb.pred)

# confusion matrix
confusionMatrix(factor(xgb.pred$label), 
                factor(xgb.pred$max_prob),
                mode = "everything")
# Reference
# https://rpubs.com/dalekube/XGBoost-Iris-Classification-Example-in-R
```


```{r}
 

LCATest %>%
  st_drop_geometry() %>%
  na.omit() %>%
  predict(LCA.xgb, .) ->
  pred
test <- st_drop_geometry(LCATest)[,c(2,4:6,8)]
test <- as.matrix(test)
pred = predict(LCA.xgb, test)
data.matrix(X_test[,-1])
result = data.frame(LCATest$SQ, pred)
print(result)

cm = confusionMatrix(LCATest$SQ, as.factor(pred))
print(cm)

```
### Using gbm package
```{r}
LCATrain$SQ = factor(LCATrain$SQ, levels = c("Low", "Moderate", "High", "Outstanding", "Unassessed", "NA"), ordered = FALSE)
set.seed(2046)
LCA_gbm <- 
  LCATrain %>%
  st_drop_geometry() %>%
  #drop_na() %>%
  gbm(SQ ~ SC + Abs + Nat + Rug,# + Rem,
      distribution = "multinomial",
      data = .,
      #weights = ,
      #var.monotone = NULL,
      n.trees = 2000,
      interaction.depth = 3,
      n.minobsinnode = 15,
      shrinkage = .005,
      #bag.fraction = 0.5, 
      #train.fraction = 1, 
      cv.folds = 30,
      #keep.data = TRUE, 
      #verbose = FALSE, 
      #class.stratify.cv = NULL,
      #n.cores = NULL
      )

#plot(LCA_gbm)
# find index for number trees with minimum CV error
best <- which.min(LCA_gbm$cv.error)

# get MSE and compute RMSE
sqrt(LCA_gbm$cv.error[best])

gbm.perf(LCA_gbm, method = "cv")

###### 4.3. validate the model
pred = predict.gbm(object = LCA_gbm,
                   newdata = LCATest,
                   n.trees = 2000,
                   type = "response")
labels = colnames(pred)[apply(pred, 1, which.max)]
LCATest <- cbind(LCATest, fitted = labels)
head(LCATest)

# setwd("/Users/Yi-Min/Rsession/ScenicOrNot/scenicness/Results/Figures")
# png(filename = "test.png", w = 10, h = 10, units = "in", res = 300)
# par(mar=c(0,0,0,0)) 
# plot(LCATest[,'fitted'], col = NA)
# dev.off()

result = data.frame(LCATest$SQ, labels)
print(result)

# confusion matrix
cm = confusionMatrix(as.factor(LCATest$SQ), as.factor(labels))
print(cm)

## Reference
# https://bradleyboehmke.github.io/HOML/gbm.html
# https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab
# https://www.datatechnotes.com/2018/03/classification-with-gradient-boosting.html
# confusion matrix terminology:https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/

```

##### predict scenicness for the Lake District

```{r, eval=FALSE}
# # predicut the scenic quality based on the predictive model of all the covariates
pred = predict.gbm(object = LCA_gbm,
                   newdata = OSGB.1km,
                   n.trees = 1000,
                   type = "response")
labels = colnames(pred)[apply(pred, 1, which.max)]
result = data.frame(LCATest$SQ, labels)
print(result)

```


##### predict the Scotland and England 
```{r, eval=FALSE}
GB.sp <-
  raster::getData("GADM", country = "United Kingdom", level = 1) %>%
  subset(NAME_1 == c("England","Scotland","Wales")) %>%
  spTransform(crs(Abs))

Abs[mask(is.na(Abs), rgeos::gBuffer(GB.sp, byid=T, width=-100))] <- 19.125

# LCA.sp <-
#   LCA %>%
#   select("UID","VS_46") %>%
#   as(., "Spatial")

# names(LCA.sp)[2] <- "SQ"

crs(OSGB.1km)
crs(sc)
crs(Abs)

OSGB.1km$Sce <- over(OSGB.1km, sc[,'Average'], fn = median) %>% as.vector() %>% unlist()
OSGB.1km$Abs <- raster::extract(Abs, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Nat <- raster::extract(Nat, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Rem <- raster::extract(Rem, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Rug <- raster::extract(Rug, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()

```


Once the package is properly installed, you can use the document class
*elsarticle* to create a manuscript. Please make sure that your
manuscript follows the guidelines in the Guide for Authors of the
relevant journal. It is not necessary to typeset your manuscript in
exactly the same way as an article, unless you are submitting to a
camera-ready copy (CRC) journal.

#### Analysis

apply the model to a case study area and suggest a character area by grouping adjacent pixels with the same class. I suppose this would have to be on a 1km grid for the Scenicness data
```{r, eval=FALSE}
st_read("/Users/Yi-Min/Rsession/ScenicOrNot/scenicness/Grid/OSGB_Grid/Shapefile/OSGB_Grid_1km.shp") %>%
  as("Spatial") %>%
  spTransform(crs(Abs)) ->
  OSGB.1km

sc %>% spTransform(crs(Abs)) -> sc

OSGB.1km$Sce <- over(OSGB.1km, sc[,'Average'], fn = median) %>% as.vector() %>% unlist()
OSGB.1km$Abs <- raster::extract(Abs, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Nat <- raster::extract(Nat, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Rem <- raster::extract(Rem, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()
OSGB.1km$Rug <- raster::extract(Rug, OSGB.1km, fun = median, na.rm = TRUE) %>% as.vector()

rgdal::writeOGR(OSGB.1km, "/Users/Yi-Min/Rsession/ScenicOrNot/scenicness/Grid/OSGB_Grid/Shapefile", "OSGB_Grid_1km_SC.shp", driver="ESRI Shapefile")

rgdal::writeOGR(GB.sp, "/Users/Yi-Min/Rsession/ScenicOrNot/scenicness/Grid/OSGB_Grid/Shapefile", "GreatBritain.shp", driver="ESRI Shapefile")

rgdal::writeOGR(test, "/Users/Yi-Min/Rsession/ScenicOrNot/scenicness/Grid/OSGB_Grid/Shapefile", "OSGB_Grid_1km_SC_new.shp", driver="ESRI Shapefile")

```

-   document style

-   baselineskip

-   front matter

-   keywords and MSC codes

-   theorems, definitions and proofs

-   lables of enumerations

-   citation style and labeling.

Front matter
============

The author names and affiliations could be formatted in two ways:

(1) Group the authors per affiliation.

(2) Use footnotes to indicate the affiliations.

See the front matter of this document for examples. You are recommended
to conform your choice to the journal you are submitting to.

Bibliography styles
===================

There are various bibliography styles available. You can select the
style of your choice in the preamble of this document. These styles are
Elsevier styles based on standard styles like Harvard and Vancouver.
Please use BibTeX to generate your bibliography and include DOIs
whenever available.

Here are two sample references: @Feynman1963118 [@Dirac1953888].

References {#references .unnumbered}
==========
